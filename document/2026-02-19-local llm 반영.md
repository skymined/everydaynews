# 2026-02-19 Local LLM 반영 작업 기록

## 1) 작업 목적
- AI Trend Digest 파이프라인의 기본 LLM 동작을 로컬 Qwen 기반으로 고정.
- 실제 로컬 LLM 추론으로 당일 리포트를 재생성.
- 논문 요약이 획일적으로 나오는 문제를 줄이기 위해 프롬프트를 정비.

## 2) 최종 결과 요약
- 기본 LLM이 `local + qwen2.5:7b`로 동작하도록 변경 완료.
- 로컬 API 엔드포인트를 `http://127.0.0.1:11434/v1`(Ollama OpenAI-compatible)로 통일.
- 깨진(인코딩 손상) 프롬프트 3개를 UTF-8 정상 프롬프트로 교체.
- `qwen2.5:7b` 실제 호출로 리포트 재생성 완료:
  - `reports/2026-02-19.md`
  - `reports/latest.md`
- 논문 섹션의 한 줄 요약/핵심 아이디어가 항목별로 구체화됨.

## 3) 변경한 파일
- `digest/models.py`
  - `LLMConfig` 기본값 변경:
    - `default_mode = "local"`
    - `local_backend = "llama"`
    - `local_base_url = "http://127.0.0.1:11434/v1"`
    - `local_model = "qwen2.5:7b"`

- `digest/llm/llama_provider.py`
  - provider 내부 기본 fallback 값 변경:
    - base_url: `http://127.0.0.1:11434/v1`
    - model: `qwen2.5:7b`

- `sources.yaml`
  - `llm` 섹션 기본값 변경:
    - `default_mode: "local"`
    - `local_backend: "llama"`
    - `local_base_url: "http://127.0.0.1:11434/v1"`
    - `local_model: "qwen2.5:7b"`

- `.env.example`
  - 로컬 기본 실행값 변경:
    - `LLM_MODE=local`
    - `LOCAL_LLM_BACKEND=llama`
    - `LOCAL_LLM_BASE_URL=http://127.0.0.1:11434/v1`
    - `LOCAL_LLM_MODEL=qwen2.5:7b`

- `README.md`
  - 기본 경로를 Ollama Qwen 기준으로 정리.
  - 로컬 실행 예시를 `ollama pull qwen2.5:7b` + `11434/v1` 기준으로 갱신.

- `prompts/news_classify.md`
- `prompts/news_summarize.md`
- `prompts/paper_summarize.md`
  - 인코딩 깨짐 문서를 UTF-8 정상 프롬프트로 교체.
  - 한국어 출력, JSON 강제, 메타 문구 금지, 논문 요약 구체성 강화 규칙 반영.

- `reports/2026-02-19.md`
- `reports/latest.md`
  - 위 변경 반영 후 실제 재생성.

## 4) 실제 수행 순서
1. 현재 설정/코드 확인 (`qwen` 기본값 반영 여부 점검).
2. 로컬 LLM 서버 상태 점검:
   - 기존 `127.0.0.1:8000/v1` 미응답 확인.
3. 런타임 설치 경로 확보:
   - `llama-cpp-python` 빌드 실패(윈도우 컴파일러 미설치) 확인.
   - 대안으로 `Ollama` 설치 후 `qwen2.5:7b` pull.
4. 코드/설정을 Ollama 엔드포인트(`11434/v1`)로 통일.
5. 실제 `qwen2.5:7b` API 호출 성공 확인.
6. `--refresh`로 리포트 재생성.
7. 리포트 결과 점검 중 프롬프트 인코딩 손상 발견.
8. 프롬프트 3개 교체 후 다시 `--refresh` 실행.
9. 리포트 정상 생성 및 `latest.md` 동기화 확인.

## 5) 발생한 문제와 해결
- 문제: `127.0.0.1:8000/v1` 로컬 서버 미동작
  - 해결: Ollama 기반(`127.0.0.1:11434/v1`)으로 전환.

- 문제: `llama-cpp-python` 설치 실패 (Windows C/C++ toolchain 없음)
  - 해결: 컴파일 의존이 적은 Ollama 사용으로 우회.

- 문제: `ollama` CLI가 즉시 PATH에서 인식되지 않음
  - 해결: `%LOCALAPPDATA%\Programs\Ollama\ollama.exe` 경로를 직접 호출.

- 문제: 생성 리포트 문구 일부 깨짐
  - 원인: `prompts/*.md` 자체가 인코딩 손상된 상태.
  - 해결: 프롬프트 파일을 UTF-8 정상 문서로 재작성 후 재생성.

- 문제: 터미널에서 한글 깨져 보이는 경우 존재
  - 원인: PowerShell 콘솔 코드페이지(cp949) 표시 문제.
  - 상태: 파일 저장은 UTF-8 정상.

## 6) 검증 결과
- 로컬 LLM 호출 검증:
  - `POST http://127.0.0.1:11434/v1/chat/completions` 다수 `200 OK`.
- 리포트 생성 성공:
  - `reports/2026-02-19.md` 생성.
  - `reports/latest.md` 동일 내용으로 갱신.
- 코드 무결성 점검:
  - `python -m compileall digest` 통과.

## 7) 남은 작업(선택)
- 필요 시 `llama.cpp server` 버전도 병행 지원 가능(현재는 Ollama 기본).
- 로컬 모델 품질 편차 대응:
  - `temperature`, `max_tokens`, 프롬프트 제약을 추가 미세조정 가능.
- 소스 확대 시 크롤링 안정화:
  - 사이트별 selector 튜닝, rate limit 정책 보완 가능.

## 8) 사용자가 직접 수정하면 되는 포인트

### A. 모델/실행 경로 변경
- `sources.yaml` 의 `llm` 섹션
  - `default_mode`, `local_base_url`, `local_model`, `default_provider` 수정.
- `.env` 또는 셸 환경변수(실행 시 우선 적용)
  - `LLM_MODE`, `LOCAL_LLM_BASE_URL`, `LOCAL_LLM_MODEL`, `LLM_PROVIDER`.

### B. 수집 사이트/키워드 변경
- `sources.yaml`
  - `sources` 목록에 사이트 추가/삭제.
  - `include_keywords`, `exclude_keywords`로 필터 조정.
  - `max_items`, `selectors`로 사이트별 수집 방식 조정.

### C. 뉴스/논문 요약 스타일 변경
- `prompts/news_classify.md`
- `prompts/news_summarize.md`
- `prompts/paper_summarize.md`
  - 분량, 톤, 금지 문구, 키워드 형식, JSON 스키마 강도를 직접 제어.

### D. 리포트 분량/표시 범위 변경
- `sources.yaml` 의 `report` 섹션
  - `max_news_items`, `max_papers`, `timezone` 조정.

### E. 요약 후처리(코드 레벨) 변경
- `digest/summarize.py`
  - LLM 실패 시 fallback 규칙
  - 캐시 재사용/재생성 로직
  - 점수 기반 선별 후처리

## 9) 보안 주의사항
- 로컬 LLM 서버는 `127.0.0.1` 바인딩 유지 권장.
  - `0.0.0.0`로 열면 외부 무인증 접근 위험.
- API Key 사용 시 `.env`/시크릿 저장소 사용, 코드/리포트에 키 직접 기록 금지.
- 외부 웹 페이지 수집 시 스크립트 실행/리다이렉트 정책을 최소 권한으로 유지.

## 10) 재실행 명령(현재 기준)
```powershell
$env:LLM_MODE='local'
$env:LOCAL_LLM_BASE_URL='http://127.0.0.1:11434/v1'
$env:LOCAL_LLM_MODEL='qwen2.5:7b'
python -m digest run --date 2026-02-19 --out reports/2026-02-19.md --refresh
```
