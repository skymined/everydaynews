# 2026-02-19 작업 수행 기록

## 1) 내가 수행한 작업
- 작업지시서(`document/ai_trend_digest_workorder.md`) 기준으로 파이프라인 전체를 신규 구현했다.
- 프로젝트 구조를 구성했다:
  - `digest/` (수집, 중복제거, 본문추출, 요약, 합성, 렌더, CLI, sqlite store)
  - `digest/llm/` (Provider 추상화, OpenAI/Gemini 구현)
  - `prompts/` (항목 요약, 일간 합성, 테마 딥다이브)
  - `sources.yaml` (소스/키워드/스코어/모델/기본값)
  - `reports/2026-02-19.md`, `reports/latest.md` 생성
  - `.github/workflows/daily.yml` 자동 실행 워크플로 추가
- 실행 가능한 CLI를 구현했다:
  - `python -m digest run --date 2026-02-19 --out reports/2026-02-19.md --skip-llm`
  - 옵션: `--provider`, `--max-items`, `--skip-llm`, `--since-hours`, `--db-path`
- sqlite 기반 중복 스킵/기록을 구현했다:
  - `seen`, `items`, `http_cache` 테이블
- 실제 실행 후 산출물 생성까지 완료했다.

## 2) 실행/검증 내역
- 의존성 설치:
  - `pip install -e .`
- 실행:
  - `python -m digest run --date 2026-02-19 --out reports/2026-02-19.md --skip-llm --db-path data/digest_final4.sqlite3`
  - 최종 DB는 `data/digest.sqlite3`로 동기화했다.
- 결과:
  - `reports/2026-02-19.md` 생성 완료
  - `reports/latest.md` 동기화 완료

## 3) 문제와 변경 사항

### 문제 A: Google HTML 스크랩에서 비기사 링크 혼입
- 증상: `fonts.gstatic.com`, 이미지 링크 등 비콘텐츠 URL이 섞임.
- 원인: HTML 구조 기반 파싱이 내비게이션/정적 자산 링크를 포함.
- 변경:
  - `sources.yaml`에서 Google 계열을 RSS 우선으로 전환
    - `google_the_keyword`: `https://blog.google/rss/`
    - `google_research`: `https://research.google/blog/rss/`
  - `digest/fetch.py`에 링크 필터(정적 자산 확장자/불필요 호스트 차단) 추가.

### 문제 B: Google Blog에서 비-AI 게시물(Pixel/Waze 등) 유입
- 증상: AI 트렌드 다이제스트에 비-AI 기사 포함.
- 원인: 일반 RSS 피드에 혼합 카테고리 게시물 존재.
- 변경:
  - `SourceConfig`에 `include_keywords`, `exclude_keywords` 필드 추가.
  - `digest/fetch.py`에 소스별 키워드 필터 적용(제목 기준 include).
  - `sources.yaml`에서 `google_the_keyword.include_keywords`로 AI 중심 필터링 설정.

### 문제 C: 일부 본문 추출 403/리다이렉트
- 증상: OpenAI 일부 페이지 `403 Forbidden`.
- 원인: 외부 사이트의 접근 정책/봇 차단.
- 변경:
  - `digest/extract.py`에서 예외를 경고 로그로 남기고 전체 파이프라인은 계속 진행하도록 처리.
  - 요약 결과에 불확실성(`unknowns`)을 남기도록 유지.

## 4) 남은 작업
- LLM 실제 연결 검증(현재 `--skip-llm`으로 생성):
  - `OPENAI_API_KEY` 또는 `GEMINI_API_KEY` 설정 후 실행 필요.
- 휴리스틱 요약 품질 개선:
  - `followup_queries`의 품질(현재는 규칙 기반) 고도화 가능.
- 운영 고도화:
  - 현재는 실행 로그를 표준 출력에만 남기므로 파일 로깅/알림(Slack, 이메일) 추가 가능.

## 5) 사용자 변경 가능 항목 (분리)

### 5.1 프롬프트 (사용자 직접 수정 권장)
- `prompts/item_summary.md`
  - 항목 요약 톤, JSON 스키마, 금지 규칙 수정
- `prompts/day_synthesis.md`
  - Top themes 형식, watchlist/follow-up 스타일 수정
- `prompts/theme_deepdive.md`
  - 리서치 플랜 질문 구조 수정

### 5.2 키워드/우선순위 (핵심 커스터마이징 포인트)
- `sources.yaml`
  - `scoring.priority_keywords`: 상위 선별 키워드
  - `scoring.tag_keywords`: 태그 분류 키워드
  - `scoring.source_weights`: 소스별 가중치
  - `sources[].include_keywords`: 소스별 포함 키워드 (현재 Google Blog에 적용)
  - `sources[].exclude_keywords`: 소스별 제외 키워드

### 5.3 수집 대상 소스 변경
- `sources.yaml`
  - `sources` 목록에서 소스 추가/삭제
  - `type: rss|html`, `url`, `max_items`, `selectors` 수정

### 5.4 모델/LLM 설정 변경
- `sources.yaml`
  - `llm.default_provider`
  - `llm.openai_model`
  - `llm.gemini_model`
  - `llm.max_items_to_summarize`
- 구현 파일:
  - `digest/llm/openai_provider.py`
  - `digest/llm/gemini_provider.py`

### 5.5 출력 형식 변경
- `digest/render.py`
  - 마크다운 섹션 순서, 항목 표현 방식, 문구 템플릿 변경

## 6) 특정 부분 수정 시 어디를 어떻게 바꾸면 되는지

### 예시 1: Google Blog에서 AI 기사만 더 강하게 제한
- 파일: `sources.yaml`
- 수정:
  - `google_the_keyword.include_keywords`에 `"artificial intelligence"`, `"foundation model"` 등 추가
  - 필요 시 `exclude_keywords`에 `"pixel"`, `"waze"` 추가

### 예시 2: Top Themes를 5개에서 3개로 줄이기
- 파일: `digest/synthesize.py`
- 수정:
  - `_fallback_synthesis`에서 `[:5]` 슬라이스를 `[:3]`으로 변경

### 예시 3: 보고서 문구/섹션명을 회사 내부 포맷에 맞추기
- 파일: `digest/render.py`
- 수정:
  - 섹션 제목(`TL;DR`, `Top Themes`, `Source-wise Highlights`) 문자열 변경

### 예시 4: 실행 시 처리량/비용 줄이기
- 파일: `sources.yaml`
- 수정:
  - `llm.max_items_to_summarize` 축소
  - `defaults.max_items_per_source` 축소
  - `sources[].max_items` 개별 축소

## 7) 보안/위험 알림
- API 키는 코드/리포지토리에 저장하지 말고 환경변수 또는 GitHub Secrets만 사용해야 한다.
  - `OPENAI_API_KEY`, `GEMINI_API_KEY`
- 워크플로(`.github/workflows/daily.yml`)의 자동 커밋은 `reports/`와 `data/digest.sqlite3`를 저장한다.
  - 공개 저장소일 경우 URL 이력/메타데이터 공개 범위를 검토해야 한다.
- 외부 사이트 접근 제한(robots/403)에 우회 시도 코드를 넣지 않았다.
  - 차단 응답은 로그 후 스킵 처리한다.
- 원문에 없는 수치 생성은 금지했고, 불확실 정보는 `unknown`/확인 필요로 남기도록 했다.


# AI Trend Digest 개선 요청서 (피드백용) - 1차

> 목적: “어제자 기준”으로 새로 올라온 **AI 트렌드/뉴스**와 **Hugging Face Daily Papers Top 10**을 **Markdown 리포트**로 간단·명확하게 정리한다.  
> 핵심은 “업데이트가 있다/없다”가 아니라 **무슨 내용인지**를 읽자마자 알 수 있게 하는 것.

---

## 1) 원하는 동작(요구사항)

### 1.1 수집 기준: “어제자”
- 기준 시간대: **Asia/Seoul(KST)**
- 수집 범위: **어제 00:00:00 ~ 23:59:59(KST)** 사이에 공개/게시된 글만 포함
  - RSS의 `published` 또는 페이지의 게시일을 우선 사용
  - 게시일 파싱이 불가능하면 `fetched_at` 기반으로 “어제 수집된 항목”으로 임시 처리하되, 이 경우 **(게시일 불명)** 표시

### 1.2 출력 형식: 2개 섹션으로 고정
- `<AI News>`: OpenAI/Google/HF Blog 등에서 “어제자 새 글” 중 **AI 트렌드로 볼 만한 것만** 선별하여 요약
- `<논문 세션>`: Hugging Face `https://huggingface.co/papers`에서 **Top 10**을 가져와 논문 요약 + 키워드 제공

---

## 2) 현재 결과의 문제점(왜 바꿔야 하는지)

### 2.1 “업데이트 확인” 메타 문구가 너무 많음
- 예: “업데이트가 확인되었습니다”, “LLM 없이 보수적으로 정리했습니다”
- **원하는 것은 파이프라인 상태 설명이 아니라, 콘텐츠 내용 요약**임  
→ 메타 문구는 전부 제거

### 2.2 형식이 과도하게 구체화되어 핵심이 안 보임
- “왜 중요?” “불확실/확인 필요” “추가 조사 쿼리” 등은 지금 단계에 불필요
- 특히 “단어 반복 분석/태그 남발” 같은 분석은 원치 않음  
→ “AI 트렌드 뉴스”로서 읽기 쉬운 요약 포맷이 우선

### 2.3 링크만 주고 ‘무슨 내용인지’가 안 보임
- 요구: 링크는 기본이지만, **본문을 클릭하기 전에** “이게 뭔 소식인지”를 10초 안에 파악 가능해야 함

---

## 3) 개선 요구(구체적인 변경 지시)

## 3.1 AI News 섹션(뉴스/공지 요약)
### 선별 규칙(간단)
- 포함: 모델/제품/API 업데이트, 연구 결과/데모, 정책/안전/규제 관련, 인프라/툴링 변화, 대형 파트너십/출시 등
- 필수 포함: openai, google, 등의 기업들 홈페이지 업데이트 내용. (주, openai, google 말고 네가 생각하는 중요 기업이 있다면 전부 넣어줘.). 만약 업데이트 내용이 없다면 미기재. 
- 제외(원칙): 일반 행사 홍보, 비AI 일반 기업 소식, 공익 캠페인(단 AI 정책/거버넌스에 직접적이면 포함)

### 각 뉴스 항목 출력 포맷(필수)
- **한 줄 헤드라인(한국어, 핵심 한 문장)**  
- `무슨 내용?` 2~4개 불릿 (사실 중심, 과장 금지)
- `왜 트렌드인가?` 1~2개 불릿 (영향/맥락 최소한)
- `링크:` 원문 URL

> 금지: “업데이트 확인됨”, “파이프라인이 LLM 없이 동작” 같은 메타, “불확실/확인 필요” 상투 문구

### 예시(원하는 스타일)
```md
- **Google.org가 ‘AI for Government Innovation’ 임팩트 챌린지 공개**
  - 링크: https://...
  - 키워드: #거버넌스 ...
  - 내용: 최대한 자세하면서도 핵심을 담을 수 있도록 기재.

## 3.2 사용자 편의성
- 매일 훝어봐야 하는 주요 기업들이나 웹사이트를 입력하면 해당 부분도 훑을 수 있도록 사용자화
- e.g. openai  웹사이트 링크를 넣으면 그때부터는 ai trend를 작성할 때 openai 웹사이트도 들어가서 업데이트 사항이 있는지 확인하고 있다면 들어가서 관련 내용 가져오기. 만약 없으면 호출 X.


# LLM 기반(로컬/클라우드) AI Trend Digest — 2차 개선 작업지시서

> 목적: 현재 **rule-based**로 동작하는 선별/요약 파트를 **LLM 기반**으로 교체한다.  
> 제공 형태:  
> - **(A) API Key 버전**: OpenAI/Gemini 등 “원격 LLM API” 사용  
> - **(B) Local Llama 버전**: **llama.cpp 기반 로컬 서버 + Qwen-7B** 사용 (권장: Qwen2.5-7B-Instruct)

---

## 0) 변경 요약(무엇이 달라지나)

### 현재(문제)
- 규칙 기반 필터링/요약 → “업데이트 확인” 메타 문구가 늘고, **내용 요약이 빈약**해짐.

### 목표(개선)
- LLM이 아래 2가지를 수행:
  1) **AI News 선별**: “AI 트렌드로 볼 만한가?” (yes/no)  
  2) **요약 생성**: “무슨 내용인지”가 바로 보이는 **헤드라인 + 불릿** 생성  
- 출력 구조는 고정:
  - `## AI News`
  - `## Papers (Hugging Face Top 10)`

---

## 1) 아키텍처 변경(코드 레벨 요구)

### 1.1 Provider 추상화(필수)
`digest/llm/base.py`에 아래 인터페이스를 정의한다.

```python
from typing import Protocol

class LLMBackend(Protocol):
    def classify_news(self, payload: dict) -> dict: ...
    def summarize_news(self, payload: dict) -> dict: ...
    def summarize_paper(self, payload: dict) -> dict: ...
```

- `payload`는 항상 **짧은 텍스트**만 포함:
  - `title`, `source`, `url`, `published_at`, `excerpt`(본문 1~2k chars 정도)
- 결과는 항상 **JSON(dict)** 로 받는다(파싱 실패 시 재시도/리페어).

### 1.2 LLM 파이프라인(권장 2단계)
- Stage 1: `classify_news` (저렴한 판별)
- Stage 2: `summarize_news` (선별된 것만 요약)

> Papers는 Top10 고정이므로 분류는 생략하고 `summarize_paper`만 수행.

---

## 2) 출력 스키마(반드시 고정)

### 2.1 AI News 요약 JSON 스키마
```json
{
  "headline_kr": "한국어 한 줄 헤드라인",
  "what": ["무슨 내용? 불릿 2~4개"],
  "why_trend": ["왜 트렌드인가? 불릿 1~2개"],
  "keywords": ["키워드 3~6개"]
}
```

### 2.2 AI News 분류 JSON 스키마
```json
{
  "include": true,
  "reason_kr": "한 문장 이유(너무 길지 않게)"
}
```

### 2.3 Paper 요약 JSON 스키마
```json
{
  "one_liner_kr": "한 줄 요약",
  "core_idea_kr": "핵심 아이디어 2~3문장",
  "keywords": ["키워드 3~6개"]
}
```

---

## 3) 프롬프트(파일로 분리)

`prompts/`에 아래 3개를 새로 만들거나 교체한다.

### 3.1 `prompts/news_classify.md`
- 입력: title/source/url/published_at/excerpt
- 출력: 위의 분류 JSON

필수 규칙:
- “파이프라인/업데이트 확인” 같은 **메타 문장 금지**
- AI 트렌드 판단 기준은 “모델/제품/API/연구/안전·정책/인프라·툴링/대형 파트너십” 중심

### 3.2 `prompts/news_summarize.md`
- 출력은 요약 JSON 스키마만
- `headline_kr`는 “원문 제목 복붙” 금지(의미를 한국어로 재작성)
- “무슨 내용?”은 사실 위주, 과장 금지

### 3.3 `prompts/paper_summarize.md`
- 입력: paper title + abstract/excerpt
- 출력: paper 요약 JSON 스키마만
- 키워드는 기술 토픽 위주(예: multimodal, agents, diffusion, RL, robotics, alignment, retrieval …)

---

## 4) (A) API Key 버전 작업지시

### 4.1 목표
- OpenAI / Gemini 등 원격 API를 통해 분류/요약 수행.
- **모델명은 코드에 하드코딩하지 말고 환경변수로 받는다.** (변경 가능성 대비)

### 4.2 환경변수(.env)
예시(필요한 것만 사용):
```bash
LLM_MODE=api
LLM_PROVIDER=openai   # openai | gemini
OPENAI_API_KEY=...
OPENAI_MODEL=...      # 사용자가 지정
GEMINI_API_KEY=...
GEMINI_MODEL=...      # 사용자가 지정
```

### 4.3 구현 요구
- `digest/llm/api_provider.py` 구현:
  - `LLM_PROVIDER=openai`면 OpenAI SDK 호출
  - `LLM_PROVIDER=gemini`면 Google GenAI SDK 호출
- **JSON 강제**:
  - 가능한 경우 “JSON mode/structured output” 사용
  - 불가능하면 “반드시 JSON만 출력” 규칙 + 파싱 실패 시 1회 재시도

### 4.4 에러/재시도 규칙(필수)
- 네트워크 실패/429/5xx → 지수 백오프 3회
- JSON 파싱 실패 → “JSON만 다시 출력” 재시도 1회
- 계속 실패하면 해당 item은 스킵하고 로그 남김(전체 파이프라인은 계속 진행)

---

## 5) (B) Local Llama 버전 작업지시 (Qwen-7B)

### 5.1 로컬 모델 선택(명확화)
- “Qwen-7B”는 최신 계열로 **Qwen2.5-7B-Instruct** 사용을 권장 (GGUF 제공). citeturn0search1turn0search0  
- llama.cpp로 실행하기 쉬운 **GGUF**를 사용한다. (예: `qwen2.5-7b-instruct-q5_k_m.gguf`) citeturn0search4

### 5.2 모델 다운로드(예시)
```bash
pip install -U "huggingface_hub[cli]"
mkdir -p models/qwen

huggingface-cli download Qwen/Qwen2.5-7B-Instruct-GGUF   qwen2.5-7b-instruct-q5_k_m.gguf   --local-dir models/qwen
```

### 5.3 로컬 서버 실행(권장: OpenAI 호환 서버)

#### 옵션 1) llama-cpp-python 서버(가장 간단)
`llama-cpp-python`은 **OpenAI API 호환 서버**를 제공한다. citeturn0search14

```bash
pip install -U "llama-cpp-python[server]"
python -m llama_cpp.server --model models/qwen/qwen2.5-7b-instruct-q5_k_m.gguf --host 0.0.0.0 --port 8000
```

#### 옵션 2) llama.cpp 바이너리 서버
- 환경에 따라 `llama-server`/`server` 바이너리로 OpenAI 호환 엔드포인트(`/v1/chat/completions`)를 제공한다.
- 단, 최신 OpenAI **Responses API(/v1/responses)** 는 아직 미지원인 경우가 있어 **/v1/chat/completions 기반**으로 구현한다. citeturn0search6turn0search2

### 5.4 애플리케이션에서 로컬 서버 호출(핵심)
- OpenAI SDK를 **로컬 `base_url`** 로 붙여 재사용한다.
- `digest/llm/llama_provider.py`에 아래 구현을 넣는다.

```python
from openai import OpenAI

class LlamaBackend:
    def __init__(self, base_url: str, model: str):
        self.client = OpenAI(base_url=base_url, api_key="local")  # 더미 키
        self.model = model

    def _chat(self, messages, temperature=0.2, max_tokens=700):
        resp = self.client.chat.completions.create(
            model=self.model,
            messages=messages,
            temperature=temperature,
            max_tokens=max_tokens,
        )
        return resp.choices[0].message.content
```

### 5.5 로컬 환경변수(.env)
```bash
LLM_MODE=local
LOCAL_LLM_BACKEND=llama
LOCAL_LLM_BASE_URL=http://127.0.0.1:8000/v1
LOCAL_LLM_MODEL=qwen2.5-7b-instruct
```

> 서버에 따라 `model` 이름이 파일명/별칭일 수 있으니 맞춰서 조정한다.

### 5.6 성능/품질 세팅 가이드(권장값)
- `temperature`: 분류 0.0~0.2 / 요약 0.2~0.4
- `max_tokens`: 뉴스 요약 300~600 / 논문 요약 400~800
- `context`: 로컬은 메모리/속도에 따라 4k~16k 권장  
  - Qwen2.5는 롱컨텍스트를 강조하지만, 런타임/서빙 스택에 따라 실사용 한계가 달라질 수 있다. citeturn0search1turn0search5

---

## 6) 공통 구현 변경 포인트(반드시 반영)

### 6.1 `summarize.py` 로직 변경
- 기존 rule-based 요약 제거 또는 fallback으로만 남김
- 흐름:
  1) (AI News 후보) `classify_news` → include=false면 제외
  2) include=true만 `summarize_news`
  3) Papers top10은 전부 `summarize_paper`

### 6.2 `render.py` 변경
- 출력은 아래 섹션만:
  - `## AI News (어제 새 글 중 트렌드)`
  - `## Papers (Hugging Face Top 10)`
- **메타 문구 금지**: “업데이트 확인”, “LLM 없이 동작” 등 출력 금지

### 6.3 캐시/재사용(비용 절감)
- 이미 요약된 URL은 sqlite에 요약 JSON 저장 후 재실행 시 재사용
- 재요약 옵션: `--refresh` 플래그로 강제 재생성 가능

---

## 7) 테스트 플랜(최소)

- 단위 테스트:
  - JSON 파서(깨진 JSON → 리페어/재시도)
  - 날짜 필터(어제 KST 범위)
  - 렌더링 포맷(섹션 2개만 존재)
- 통합 테스트:
  - `LLM_MODE=api`로 1회 실행
  - `LLM_MODE=local` + llama 서버 띄워서 1회 실행
- 성공 기준:
  - 뉴스 5~15개(어제자 중 트렌드만) + papers 10개
  - 각 항목이 “무슨 내용인지” 바로 보임

---

## 8) Done 정의(완료 조건)

- [ ] `LLM_MODE=api` / `LLM_MODE=local` 둘 다 동작
- [ ] local은 **Qwen-7B (Qwen2.5-7B-Instruct GGUF)** 로 재현 가능
- [ ] 출력 리포트에 메타 문구 없음(내용 요약 중심)
- [ ] Papers 항목마다 “한 줄 요약 + 핵심 아이디어 + 키워드” 포함
- [ ] 실패한 항목이 있어도 전체 리포트 생성은 완료됨(로그에만 남김)

---

## 2차 작업 반영 결과 (2026-02-19)

- `LLMBackend` 인터페이스로 교체:
  - `classify_news(payload)`
  - `summarize_news(payload)`
  - `summarize_paper(payload)`
- LLM 백엔드 구현:
  - API: `digest/llm/api_provider.py` (OpenAI/Gemini, JSON 파싱 실패 1회 재시도, 네트워크 백오프 재시도)
  - Local: `digest/llm/llama_provider.py` (llama.cpp OpenAI 호환 서버 `/v1/chat/completions`)
  - 팩토리: `digest/llm/factory.py` (`LLM_MODE=api|local`)
- 프롬프트 분리:
  - `prompts/news_classify.md`
  - `prompts/news_summarize.md`
  - `prompts/paper_summarize.md`
- 파이프라인 변경:
  - 뉴스: `classify_news` 통과 항목만 `summarize_news`
  - 논문: HF Top10을 `summarize_paper`
  - 캐시 재사용: sqlite `items.json_summary` 사용
  - 강제 재생성: `--refresh` 옵션 추가
- 렌더 고정:
  - `## AI News`
  - `## Papers (Hugging Face Top 10)`
- 검증:
  - `python -m unittest discover -s tests -p "test_*.py"` 통과
  - `python -m digest run --date 2026-02-19 --out reports/2026-02-19.md --skip-llm --refresh` 실행/리포트 생성 완료
